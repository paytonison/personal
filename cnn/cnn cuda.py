# -*- coding: utf-8 -*-
"""scratchpad

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/empty.ipynb
"""

!pip install datasets

from datasets import load_dataset

# Load the IMDb dataset
dataset = load_dataset('imdb')

from transformers import AutoTokenizer

# Load a tokenizer (we can use a pre-trained tokenizer from BERT, GPT, etc.)
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# Tokenize the dataset (this step converts text into word indices)
def tokenize_data(example):
    return tokenizer(example['text'], padding='max_length', truncation=True, max_length=256)

# Apply the tokenizer to the dataset
tokenized_dataset = dataset.map(tokenize_data, batched=True)

# Remove unnecessary columns and rename the 'label' column
tokenized_dataset = tokenized_dataset.remove_columns(['text']).rename_column('label', 'labels')
tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])

from torch.utils.data import DataLoader

# Create DataLoader for training and testing
train_loader = DataLoader(tokenized_dataset['train'], batch_size=32, shuffle=True)
test_loader = DataLoader(tokenized_dataset['test'], batch_size=32, shuffle=False)

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

class TextCNN(nn.Module):
    def __init__(self, vocab_size, embed_size, num_classes):
        super(TextCNN, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)

        # Convolutional layers for n-grams (3, 4, and 5 words)
        self.conv1 = nn.Conv2d(1, 100, (3, embed_size), padding=(1, 0))
        self.conv2 = nn.Conv2d(1, 100, (4, embed_size), padding=(2, 0))
        self.conv3 = nn.Conv2d(1, 100, (5, embed_size), padding=(2, 0))

        # Fully connected layers
        self.fc1 = nn.Linear(300, 128)
        self.fc2 = nn.Linear(128, num_classes)  # num_classes = 2 for binary classification

    def forward(self, x):
        x = self.embedding(x).unsqueeze(1)  # Add channel dimension

        # Apply convolutional layers + ReLU
        conv1 = torch.relu(self.conv1(x)).squeeze(3)
        conv2 = torch.relu(self.conv2(x)).squeeze(3)
        conv3 = torch.relu(self.conv3(x)).squeeze(3)

        # Max pooling
        pooled1 = F.max_pool1d(conv1, conv1.size(2)).squeeze(2)
        pooled2 = F.max_pool1d(conv2, conv2.size(2)).squeeze(2)
        pooled3 = F.max_pool1d(conv3, conv3.size(2)).squeeze(2)

        # Concatenate pooled results and pass through fully connected layers
        out = torch.cat([pooled1, pooled2, pooled3], dim=1)
        out = torch.relu(self.fc1(out))
        out = self.fc2(out)  # No activation for logits

        return out

# Hyperparameters
vocab_size = tokenizer.vocab_size
embed_size = 128  # Size of word embeddings
num_classes = 2   # Binary classification (positive/negative sentiment)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Initialize model, loss function, and optimizer
model = TextCNN(vocab_size, embed_size, num_classes).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)

# Training loop
# Training loop
num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    for batch_idx, batch in enumerate(train_loader):
        # Move data and target to the GPU
        data = batch['input_ids'].to(device)
        target = batch['labels'].to(device)

        # Forward pass
        output = model(data)
        loss = criterion(output, target)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch_idx % 100 == 0:
            print(f'Epoch {epoch+1}/{num_epochs}, Step {batch_idx}, Loss: {loss.item():.4f}')

# Evaluate the model
model.eval()
correct = 0
total = 0

with torch.no_grad():
    for batch in test_loader:
        # Move data and target to the GPU
        data = batch['input_ids'].to(device)
        target = batch['labels'].to(device)

        # Forward pass
        outputs = model(data)

        # Get the predicted class
        _, predicted = torch.max(outputs.data, 1)

        total += target.size(0)
        correct += (predicted == target).sum().item()

accuracy = 100 * correct / total
print(f'Test Accuracy: {accuracy:.2f}%')