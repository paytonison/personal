# -*- coding: utf-8 -*-
"""scratchpad

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/empty.ipynb
"""

!pip install datasets

from datasets import load_dataset

# Load the IMDb dataset
dataset = load_dataset('imdb')

from transformers import AutoTokenizer

# Load a tokenizer (we can use a pre-trained tokenizer from BERT, GPT, etc.)
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# Tokenize the dataset (this step converts text into word indices)
def tokenize_data(example):
    return tokenizer(example['text'], padding='max_length', truncation=True, max_length=256)

# Apply the tokenizer to the dataset
tokenized_dataset = dataset.map(tokenize_data, batched=True)

# Remove unnecessary columns and rename the 'label' column
tokenized_dataset = tokenized_dataset.remove_columns(['text']).rename_column('label', 'labels')
tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])

from torch.utils.data import DataLoader

# Create DataLoader for training and testing
train_loader = DataLoader(tokenized_dataset['train'], batch_size=32, shuffle=True)
test_loader = DataLoader(tokenized_dataset['test'], batch_size=32, shuffle=False)

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.nn import TransformerEncoder, TransformerEncoderLayer

class HybridTransformerCNN(nn.Module):
    def __init__(self, vocab_size, embed_size=512, num_classes=2, num_heads=8, num_layers=2):
        super(HybridTransformerCNN, self).__init__()

        # Embedding layer
        self.embedding = nn.Embedding(vocab_size, embed_size)

        # Transformer encoder layer
        transformer_layer = TransformerEncoderLayer(d_model=embed_size, nhead=num_heads)
        self.transformer = TransformerEncoder(transformer_layer, num_layers=num_layers)

        # Convolutional layers
        self.conv1 = nn.Conv2d(1, 256, (3, embed_size), padding=(1, 0))  # First conv layer
        self.batch_norm1 = nn.BatchNorm2d(256)  # Batch normalization after conv1

        self.conv2 = nn.Conv2d(256, 512, (3, 1), padding=(1, 0))  # Second conv layer
        self.batch_norm2 = nn.BatchNorm2d(512)  # Batch normalization after conv2

        self.pool = nn.MaxPool2d((2, 1))  # Max pooling layer

        # Fully connected layers
        self.fc1 = nn.Linear(65536, 512)  # Adjust size after pooling
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, num_classes)

    def forward(self, x):
        # Embedding
        x = self.embedding(x)  # (batch_size, seq_length, embed_size)

        # Transformer encoder
        x = self.transformer(x)  # Transformer expects input of shape (seq_length, batch_size, embed_size)
        x = x.unsqueeze(1)  # Add channel dimension for CNN (batch_size, 1, seq_length, embed_size)

        # Convolutional layers + normalization + pooling
        x = torch.relu(self.batch_norm1(self.conv1(x)))  # Convolution + batch norm
        x = torch.relu(self.batch_norm2(self.conv2(x)))  # Convolution + batch norm
        x = self.pool(x)  # Apply pooling

        # Flatten for fully connected layers
        x = x.view(x.size(0), -1)

        # Fully connected layers
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)  # No activation here (logits)

        return x

# Hyperparameters
vocab_size = tokenizer.vocab_size
embed_size = 256
num_heads = 32
num_layers = 16
num_classes = 2

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = HybridTransformerCNN(vocab_size, embed_size, num_classes, num_heads, num_layers).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-5)

# Training loop
# Training loop
num_epochs = 15
for epoch in range(num_epochs):
    model.train()
    for batch_idx, batch in enumerate(train_loader):
        # Move data and target to the GPU
        data = batch['input_ids'].to(device)
        target = batch['labels'].to(device)

        # Forward pass
        output = model(data)
        loss = criterion(output, target)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch_idx % 100 == 0:
            print(f'Epoch {epoch+1}/{num_epochs}, Step {batch_idx}, Loss: {loss.item():.4f}')

# Evaluate the model
model.eval()
correct = 0
total = 0

with torch.no_grad():
    for batch in test_loader:
        # Move data and target to the GPU
        data = batch['input_ids'].to(device)
        target = batch['labels'].to(device)

        # Forward pass
        outputs = model(data)

        # Get the predicted class
        _, predicted = torch.max(outputs.data, 1)

        total += target.size(0)
        correct += (predicted == target).sum().item()

accuracy = 100 * correct / total
print(f'Test Accuracy: {accuracy:.2f}%')